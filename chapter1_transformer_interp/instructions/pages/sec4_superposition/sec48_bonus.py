import streamlit as st

def section():
    st.sidebar.markdown(r"""
## Table of Contents

<ul class="contents">
    <li class='margtop'><a class='contents-el' href='#suggested-paper-replications'>Suggested paper replications</a></li>
    <li class='margtop'><a class='contents-el' href='#suggested-topics-for-further-exploration'>Suggested topics for further exploration</a></li>
</ul>""", unsafe_allow_html=True)
    
    st.markdown(
r"""
# Bonus

## Suggested paper replications

<br>

### [Toy Models of Superposition](https://transformer-circuits.pub/2022/toy_model/index.html#phase-change)

There are several aspects of this paper which we didn't cover in these exercises. In particular, **superposition as a phase change** studies the interaction between sparsity and relative feature importance, and finds a phase change in the optimal weight configuration as these inputs are varied. Some examples can be found in [this notebook](https://github.com/wattenberg/superposition/blob/main/Exploring_Exact_Toy_Models.ipynb)

This might be a good replication for you if:

* You enjoy diving into the specific mathematical details of superposition
* You find theoretical work interesting, as well as empirical work
* You've enjoyed the first ~3 exercise sets in this section

<br>

### [Polysemanticity and Capacity in Neural Networks](https://arxiv.org/pdf/2210.01892.pdf)

This is a paper by Redwood Research, which builds on the ideas we discussed in the first three four of this paper (toy models of superposition, and the results on feature geometry).

They deeply study a measure called capacity, which is the same as what we called dimensionality above. Their results suggest an explanation for why features are often sharply "pinned" to either 0 or 1 capacity (i.e. not represented at all, or represented orthogonally to all other features).

This might be a good replication for you if:

* You enjoy diving into the specific mathematical details of superposition
* You're comfortable with mathematical topics like linear algebra and calculus
* You find theoretical work interesting, as well as empirical work
* You've enjoyed the first ~4 exercise sets in this section

<br>

## Suggested topics for further exploration

<br>

### [Gated SAEs](https://arxiv.org/pdf/2404.16014)

Sparse autoencoders suffer from a problem called [shrinkage](https://www.alignmentforum.org/posts/3JuSjTZyMzaSeTxKk/addressing-feature-suppression-in-saes). Recall that the actual objective we want is that the L0 "norm" (the number of non-zero elements) of the hidden layer is small, and we use the L1 norm as a proxy for this. The two loss term in the SAE loss function have conflicting goals: the reconstruction term wants to make the autoencoder good at reconstructing the input, and the sparsity term wants to shrink the magnitude of the hidden layer. This means that even when perfect reconstruction is possible with only a single hidden unit activated, the sparsity loss will bias the magnitude of this hidden unit to zero, and the reconstruction
will be worse.  

One proposed solution is Gated SAEs (see [Section 3.2](https://arxiv.org/pdf/2404.16014#page=5)).
 
The Gated SAE (Equation 6) separates the sparsity and reconstruction objectives, by having a separate gate network which decides which hidden units to activate. 
$$
\text{GSAE}(z) = \mathbf{1}\left(((z - b_{\text{dec}}) W_{\text{gate}} + b_{\text{gate}}) > 0\right) \odot \text{ReLU}((z - b_{\text{dec}}) W_{\text{mag}} + b_{\text{mag}}) W_{\text{dec}} + b_{\text{dec}},
$$
where $\mathbf{1}[\cdot > 0]$ is the pointwise Heavisde step function and $\odot$ is elementwise multiplication. The features' gate and activation magnitudes are computed by sharing weight matrices, $W_{\text{mag}}[j,i] = W_{\text{gate}}[i,j] \text{exp}(r[j])$, where $r$ is a learned rescaling vector. 

GSAEs have been shown to Pareto dominate SAEs (a lower reconstruction loss with lower sparsity!)

See [Equation 8](https://arxiv.org/pdf/2404.16014#page=2.70) for the loss function to train an GSAE against, and [Appendix G](https://arxiv.org/pdf/2404.16014#page=34) for pseudocode to define the GSAE.

<!--TODO: Add solutions for the code to define and train a GSAE.-->

Why do you think we don't just train against the L0 "norm" directly? (Also, why do I keep writing <b>norm</b> in quotes when referring to the L0 "norm"?)

<details>
<summary>Hint</summary>

Think about what it means to train against a loss function or objective function, in terms of gradient descent.

</details>

<details>
<summary>Answer</summary>

The L0 "norm" isn't even a norm by the mathematical definition of a norm (it fails the property $||2x||_0 = 2||x||_0$). But most importantly for our purposes, its derivative is zero everywhere it is defined, which would give us no information about how to update the weights if we tried to perform gradient descent on it. The L1 norm is (in a well-defined sense) the [best](https://www.cs.utep.edu/vladik/2013/tr13-18.pdf) convex approximation of the L0 "norm" and it also has the property of encouraging sparsity, so we use it as a proxy.

</details>

<br>

### [Softmax Linear Units](https://transformer-circuits.pub/2022/solu/index.html)

This is a proposed architectural change which appears to increase the number of interpretable MLPs with low performance cost. In particular, it may reduce the instance of superposition.

TL;DR: SOLU is an activation function $\vec{x} \to \vec{x} * \operatorname{softmax}(\vec{x})$ which encourages sparsity in activations in the same way that softmax encourages sparsity (often softmax'ed probability distributions have one probability close to one and the others close to zero). Encouraging activation sparsity might make it harder for neurons to be polysemantic.

Replication of the results of this paper might not be a practical final week project. However, several transformers in the TransformerLens library have been trained with SOLU (see the [model page](https://neelnanda-io.github.io/TransformerLens/generated/model_properties_table.html) for more details), which makes them a good candidate for closer study. Some questions you might want to explore:

- How do Neel's SoLU and GELU models compare in [neuroscope](https://neuroscope.io/) under the polysemanticity metric used in the SoLU paper? (what fraction of neurons seem monosemantic when looking at the top 10 activating dataset examples for 1 minute)
- The SoLU metrics for polysemanticity are somewhat limited, since they only provide information about whether a neuron is monosemantic when activating strongly (and this may not be corrrelated to whether it is monosemantic in general - see [this caveat](https://transformer-circuits.pub/2022/solu/index.html#:~:text=be%20reverse%2Dengineered.-,CAVEAT,-Since%20publication%2C%20we%27ve) in the paper). Can you find any better metrics? Can you be more reliable, or more scalable? 
- The paper [speculates](https://transformer-circuits.pub/2022/solu/index.html#section-4-3) that the LayerNorm after the SoLU activations lets the model "smuggle through" superposition, by smearing features across many dimensions, having the output be very small, and letting the LayerNorm scale it up. Can you find any evidence of this in solu-1l?

<br>

### [Towards Monosemanticity: Decomposing Language Models With Dictionary Learning](https://transformer-circuits.pub/2023/monosemantic-features/index.html)

There are many other interesting topics from Anthropic's dictionary learning paper which we didn't have time to dive into here, such as automated interpretability, feature motifs, and finite-state automata.

There's also a [Future Work](https://transformer-circuits.pub/2023/monosemantic-features/index.html#discussion-future-work) section at the end, which readers might find interesting for any project ideas!

<br>

### [Exciting Open Problems In Mech Interp v2](https://docs.google.com/document/d/1lIIzMjenXh-U0j5jkuqSDTawCoMNW4TqUlxk7mmbmRg/edit)

This document was written by Neel, and it collates a bunch of interesting open problems in mech interp (with a strong focus on SAE-related ones). Again, many of these could make great capstone projects! We encourage you to pick more achievable, less ambitious projects from this list though.

If any of the projects you're interested in involve training a sparse autoencoder, we *strongly* recommend [this post](https://www.lesswrong.com/posts/fifPCos6ddsmJYahD/my-best-guess-at-the-important-tricks-for-training-1l-saes) by Arthur Conmy, which collates a bunch of different techniques for training SAEs well (most of which we didn't cover in these exercises).

""", unsafe_allow_html=True)