{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "945197a1-5d92-4b34-8033-72f3d87c1ea2",
   "metadata": {},
   "source": [
    "BPE (Byte pair encoding) Guide:\n",
    "\n",
    "1. Train:\n",
    "    1. get text\n",
    "    2. initialize vocab - map id: bytes. Start values are range 0-255: byte([0-255])\n",
    "    3. initialize merges - map new_token_id: (prev_token_i, prev_token_j) - map of new token that formed from merge two existing tokens\n",
    "    4. while vocab_size < target_vocab_size:\n",
    "       - get frequencies of i, i+1 tokens. output: dict cat(token_i, token_j): freq\n",
    "       - get token pair with max frequency\n",
    "       - add to merges new_token_id: (token_i, token_j)\n",
    "       - replace all max freq token pair with new_token_id\n",
    "    5. for (token_i, token_j), new_token in merges:\n",
    "       - add to vocab new_token = vocab\\[token_i\\] + vocab\\[token_j\\]\n",
    "\n",
    "1. Inference:\n",
    "    1. Encode:\n",
    "       - encode text to byte tokens\n",
    "       - while len(tokens) > 2 or no more merges performed: if pair tokens in merges: replace\n",
    "    2. Decode: go through tokens and for each token get bytes from vocab: str(vocab\\[token\\])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "470d31c8-6094-444d-ad4c-b7c611e151b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "485f815660d64104a2267efe0c14f70a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "self = BasicTokenizer()\n",
    "vocab_size = 260\n",
    "verbose = True\n",
    "text = \"hello world!\"\n",
    "tokens = list(text.encode('utf-8'))\n",
    "vocab_size -= 256\n",
    "\n",
    "merges = {}\n",
    "\n",
    "progress_bar = tqdm(range(vocab_size)) if verbose else range(vocab_size)\n",
    "\n",
    "for i in progress_bar:\n",
    "    stats = self.get_freqs(tokens)\n",
    "    max_freq_pair, _ = max(stats.items(), key=lambda x: x[1])\n",
    "    new_token_id = 256 + i\n",
    "    merges[new_token_id] = max_freq_pair\n",
    "    tokens = self.replace_tokens_by_parent(tokens, max_freq_pair, new_token_id)\n",
    "\n",
    "inv_merges = {v: k for k, v in merges.items()}\n",
    "vocab = {i: bytes([i]) for i in range(256)}\n",
    "\n",
    "for k, (p0, p1) in merges.items():\n",
    "    vocab[k] = vocab[p0] + vocab[p1]\n",
    "\n",
    "self.vocab, self.merges, self.inv_merges = vocab, merges, inv_merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "54ef5a9b-bcaf-4512-982a-140288c22727",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: b'\\x00',\n",
       " 1: b'\\x01',\n",
       " 2: b'\\x02',\n",
       " 3: b'\\x03',\n",
       " 4: b'\\x04',\n",
       " 5: b'\\x05',\n",
       " 6: b'\\x06',\n",
       " 7: b'\\x07',\n",
       " 8: b'\\x08',\n",
       " 9: b'\\t',\n",
       " 10: b'\\n',\n",
       " 11: b'\\x0b',\n",
       " 12: b'\\x0c',\n",
       " 13: b'\\r',\n",
       " 14: b'\\x0e',\n",
       " 15: b'\\x0f',\n",
       " 16: b'\\x10',\n",
       " 17: b'\\x11',\n",
       " 18: b'\\x12',\n",
       " 19: b'\\x13',\n",
       " 20: b'\\x14',\n",
       " 21: b'\\x15',\n",
       " 22: b'\\x16',\n",
       " 23: b'\\x17',\n",
       " 24: b'\\x18',\n",
       " 25: b'\\x19',\n",
       " 26: b'\\x1a',\n",
       " 27: b'\\x1b',\n",
       " 28: b'\\x1c',\n",
       " 29: b'\\x1d',\n",
       " 30: b'\\x1e',\n",
       " 31: b'\\x1f',\n",
       " 32: b' ',\n",
       " 33: b'!',\n",
       " 34: b'\"',\n",
       " 35: b'#',\n",
       " 36: b'$',\n",
       " 37: b'%',\n",
       " 38: b'&',\n",
       " 39: b\"'\",\n",
       " 40: b'(',\n",
       " 41: b')',\n",
       " 42: b'*',\n",
       " 43: b'+',\n",
       " 44: b',',\n",
       " 45: b'-',\n",
       " 46: b'.',\n",
       " 47: b'/',\n",
       " 48: b'0',\n",
       " 49: b'1',\n",
       " 50: b'2',\n",
       " 51: b'3',\n",
       " 52: b'4',\n",
       " 53: b'5',\n",
       " 54: b'6',\n",
       " 55: b'7',\n",
       " 56: b'8',\n",
       " 57: b'9',\n",
       " 58: b':',\n",
       " 59: b';',\n",
       " 60: b'<',\n",
       " 61: b'=',\n",
       " 62: b'>',\n",
       " 63: b'?',\n",
       " 64: b'@',\n",
       " 65: b'A',\n",
       " 66: b'B',\n",
       " 67: b'C',\n",
       " 68: b'D',\n",
       " 69: b'E',\n",
       " 70: b'F',\n",
       " 71: b'G',\n",
       " 72: b'H',\n",
       " 73: b'I',\n",
       " 74: b'J',\n",
       " 75: b'K',\n",
       " 76: b'L',\n",
       " 77: b'M',\n",
       " 78: b'N',\n",
       " 79: b'O',\n",
       " 80: b'P',\n",
       " 81: b'Q',\n",
       " 82: b'R',\n",
       " 83: b'S',\n",
       " 84: b'T',\n",
       " 85: b'U',\n",
       " 86: b'V',\n",
       " 87: b'W',\n",
       " 88: b'X',\n",
       " 89: b'Y',\n",
       " 90: b'Z',\n",
       " 91: b'[',\n",
       " 92: b'\\\\',\n",
       " 93: b']',\n",
       " 94: b'^',\n",
       " 95: b'_',\n",
       " 96: b'`',\n",
       " 97: b'a',\n",
       " 98: b'b',\n",
       " 99: b'c',\n",
       " 100: b'd',\n",
       " 101: b'e',\n",
       " 102: b'f',\n",
       " 103: b'g',\n",
       " 104: b'h',\n",
       " 105: b'i',\n",
       " 106: b'j',\n",
       " 107: b'k',\n",
       " 108: b'l',\n",
       " 109: b'm',\n",
       " 110: b'n',\n",
       " 111: b'o',\n",
       " 112: b'p',\n",
       " 113: b'q',\n",
       " 114: b'r',\n",
       " 115: b's',\n",
       " 116: b't',\n",
       " 117: b'u',\n",
       " 118: b'v',\n",
       " 119: b'w',\n",
       " 120: b'x',\n",
       " 121: b'y',\n",
       " 122: b'z',\n",
       " 123: b'{',\n",
       " 124: b'|',\n",
       " 125: b'}',\n",
       " 126: b'~',\n",
       " 127: b'\\x7f',\n",
       " 128: b'\\x80',\n",
       " 129: b'\\x81',\n",
       " 130: b'\\x82',\n",
       " 131: b'\\x83',\n",
       " 132: b'\\x84',\n",
       " 133: b'\\x85',\n",
       " 134: b'\\x86',\n",
       " 135: b'\\x87',\n",
       " 136: b'\\x88',\n",
       " 137: b'\\x89',\n",
       " 138: b'\\x8a',\n",
       " 139: b'\\x8b',\n",
       " 140: b'\\x8c',\n",
       " 141: b'\\x8d',\n",
       " 142: b'\\x8e',\n",
       " 143: b'\\x8f',\n",
       " 144: b'\\x90',\n",
       " 145: b'\\x91',\n",
       " 146: b'\\x92',\n",
       " 147: b'\\x93',\n",
       " 148: b'\\x94',\n",
       " 149: b'\\x95',\n",
       " 150: b'\\x96',\n",
       " 151: b'\\x97',\n",
       " 152: b'\\x98',\n",
       " 153: b'\\x99',\n",
       " 154: b'\\x9a',\n",
       " 155: b'\\x9b',\n",
       " 156: b'\\x9c',\n",
       " 157: b'\\x9d',\n",
       " 158: b'\\x9e',\n",
       " 159: b'\\x9f',\n",
       " 160: b'\\xa0',\n",
       " 161: b'\\xa1',\n",
       " 162: b'\\xa2',\n",
       " 163: b'\\xa3',\n",
       " 164: b'\\xa4',\n",
       " 165: b'\\xa5',\n",
       " 166: b'\\xa6',\n",
       " 167: b'\\xa7',\n",
       " 168: b'\\xa8',\n",
       " 169: b'\\xa9',\n",
       " 170: b'\\xaa',\n",
       " 171: b'\\xab',\n",
       " 172: b'\\xac',\n",
       " 173: b'\\xad',\n",
       " 174: b'\\xae',\n",
       " 175: b'\\xaf',\n",
       " 176: b'\\xb0',\n",
       " 177: b'\\xb1',\n",
       " 178: b'\\xb2',\n",
       " 179: b'\\xb3',\n",
       " 180: b'\\xb4',\n",
       " 181: b'\\xb5',\n",
       " 182: b'\\xb6',\n",
       " 183: b'\\xb7',\n",
       " 184: b'\\xb8',\n",
       " 185: b'\\xb9',\n",
       " 186: b'\\xba',\n",
       " 187: b'\\xbb',\n",
       " 188: b'\\xbc',\n",
       " 189: b'\\xbd',\n",
       " 190: b'\\xbe',\n",
       " 191: b'\\xbf',\n",
       " 192: b'\\xc0',\n",
       " 193: b'\\xc1',\n",
       " 194: b'\\xc2',\n",
       " 195: b'\\xc3',\n",
       " 196: b'\\xc4',\n",
       " 197: b'\\xc5',\n",
       " 198: b'\\xc6',\n",
       " 199: b'\\xc7',\n",
       " 200: b'\\xc8',\n",
       " 201: b'\\xc9',\n",
       " 202: b'\\xca',\n",
       " 203: b'\\xcb',\n",
       " 204: b'\\xcc',\n",
       " 205: b'\\xcd',\n",
       " 206: b'\\xce',\n",
       " 207: b'\\xcf',\n",
       " 208: b'\\xd0',\n",
       " 209: b'\\xd1',\n",
       " 210: b'\\xd2',\n",
       " 211: b'\\xd3',\n",
       " 212: b'\\xd4',\n",
       " 213: b'\\xd5',\n",
       " 214: b'\\xd6',\n",
       " 215: b'\\xd7',\n",
       " 216: b'\\xd8',\n",
       " 217: b'\\xd9',\n",
       " 218: b'\\xda',\n",
       " 219: b'\\xdb',\n",
       " 220: b'\\xdc',\n",
       " 221: b'\\xdd',\n",
       " 222: b'\\xde',\n",
       " 223: b'\\xdf',\n",
       " 224: b'\\xe0',\n",
       " 225: b'\\xe1',\n",
       " 226: b'\\xe2',\n",
       " 227: b'\\xe3',\n",
       " 228: b'\\xe4',\n",
       " 229: b'\\xe5',\n",
       " 230: b'\\xe6',\n",
       " 231: b'\\xe7',\n",
       " 232: b'\\xe8',\n",
       " 233: b'\\xe9',\n",
       " 234: b'\\xea',\n",
       " 235: b'\\xeb',\n",
       " 236: b'\\xec',\n",
       " 237: b'\\xed',\n",
       " 238: b'\\xee',\n",
       " 239: b'\\xef',\n",
       " 240: b'\\xf0',\n",
       " 241: b'\\xf1',\n",
       " 242: b'\\xf2',\n",
       " 243: b'\\xf3',\n",
       " 244: b'\\xf4',\n",
       " 245: b'\\xf5',\n",
       " 246: b'\\xf6',\n",
       " 247: b'\\xf7',\n",
       " 248: b'\\xf8',\n",
       " 249: b'\\xf9',\n",
       " 250: b'\\xfa',\n",
       " 251: b'\\xfb',\n",
       " 252: b'\\xfc',\n",
       " 253: b'\\xfd',\n",
       " 254: b'\\xfe',\n",
       " 255: b'\\xff',\n",
       " 256: b'he',\n",
       " 257: b'hel',\n",
       " 258: b'hell',\n",
       " 259: b'hello'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e3959a7b-cddb-466c-9a8a-ad2a07cb268e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: b'\\x00',\n",
       " 1: b'\\x01',\n",
       " 2: b'\\x02',\n",
       " 3: b'\\x03',\n",
       " 4: b'\\x04',\n",
       " 5: b'\\x05',\n",
       " 6: b'\\x06',\n",
       " 7: b'\\x07',\n",
       " 8: b'\\x08',\n",
       " 9: b'\\t',\n",
       " 10: b'\\n',\n",
       " 11: b'\\x0b',\n",
       " 12: b'\\x0c',\n",
       " 13: b'\\r',\n",
       " 14: b'\\x0e',\n",
       " 15: b'\\x0f',\n",
       " 16: b'\\x10',\n",
       " 17: b'\\x11',\n",
       " 18: b'\\x12',\n",
       " 19: b'\\x13',\n",
       " 20: b'\\x14',\n",
       " 21: b'\\x15',\n",
       " 22: b'\\x16',\n",
       " 23: b'\\x17',\n",
       " 24: b'\\x18',\n",
       " 25: b'\\x19',\n",
       " 26: b'\\x1a',\n",
       " 27: b'\\x1b',\n",
       " 28: b'\\x1c',\n",
       " 29: b'\\x1d',\n",
       " 30: b'\\x1e',\n",
       " 31: b'\\x1f',\n",
       " 32: b' ',\n",
       " 33: b'!',\n",
       " 34: b'\"',\n",
       " 35: b'#',\n",
       " 36: b'$',\n",
       " 37: b'%',\n",
       " 38: b'&',\n",
       " 39: b\"'\",\n",
       " 40: b'(',\n",
       " 41: b')',\n",
       " 42: b'*',\n",
       " 43: b'+',\n",
       " 44: b',',\n",
       " 45: b'-',\n",
       " 46: b'.',\n",
       " 47: b'/',\n",
       " 48: b'0',\n",
       " 49: b'1',\n",
       " 50: b'2',\n",
       " 51: b'3',\n",
       " 52: b'4',\n",
       " 53: b'5',\n",
       " 54: b'6',\n",
       " 55: b'7',\n",
       " 56: b'8',\n",
       " 57: b'9',\n",
       " 58: b':',\n",
       " 59: b';',\n",
       " 60: b'<',\n",
       " 61: b'=',\n",
       " 62: b'>',\n",
       " 63: b'?',\n",
       " 64: b'@',\n",
       " 65: b'A',\n",
       " 66: b'B',\n",
       " 67: b'C',\n",
       " 68: b'D',\n",
       " 69: b'E',\n",
       " 70: b'F',\n",
       " 71: b'G',\n",
       " 72: b'H',\n",
       " 73: b'I',\n",
       " 74: b'J',\n",
       " 75: b'K',\n",
       " 76: b'L',\n",
       " 77: b'M',\n",
       " 78: b'N',\n",
       " 79: b'O',\n",
       " 80: b'P',\n",
       " 81: b'Q',\n",
       " 82: b'R',\n",
       " 83: b'S',\n",
       " 84: b'T',\n",
       " 85: b'U',\n",
       " 86: b'V',\n",
       " 87: b'W',\n",
       " 88: b'X',\n",
       " 89: b'Y',\n",
       " 90: b'Z',\n",
       " 91: b'[',\n",
       " 92: b'\\\\',\n",
       " 93: b']',\n",
       " 94: b'^',\n",
       " 95: b'_',\n",
       " 96: b'`',\n",
       " 97: b'a',\n",
       " 98: b'b',\n",
       " 99: b'c',\n",
       " 100: b'd',\n",
       " 101: b'e',\n",
       " 102: b'f',\n",
       " 103: b'g',\n",
       " 104: b'h',\n",
       " 105: b'i',\n",
       " 106: b'j',\n",
       " 107: b'k',\n",
       " 108: b'l',\n",
       " 109: b'm',\n",
       " 110: b'n',\n",
       " 111: b'o',\n",
       " 112: b'p',\n",
       " 113: b'q',\n",
       " 114: b'r',\n",
       " 115: b's',\n",
       " 116: b't',\n",
       " 117: b'u',\n",
       " 118: b'v',\n",
       " 119: b'w',\n",
       " 120: b'x',\n",
       " 121: b'y',\n",
       " 122: b'z',\n",
       " 123: b'{',\n",
       " 124: b'|',\n",
       " 125: b'}',\n",
       " 126: b'~',\n",
       " 127: b'\\x7f',\n",
       " 128: b'\\x80',\n",
       " 129: b'\\x81',\n",
       " 130: b'\\x82',\n",
       " 131: b'\\x83',\n",
       " 132: b'\\x84',\n",
       " 133: b'\\x85',\n",
       " 134: b'\\x86',\n",
       " 135: b'\\x87',\n",
       " 136: b'\\x88',\n",
       " 137: b'\\x89',\n",
       " 138: b'\\x8a',\n",
       " 139: b'\\x8b',\n",
       " 140: b'\\x8c',\n",
       " 141: b'\\x8d',\n",
       " 142: b'\\x8e',\n",
       " 143: b'\\x8f',\n",
       " 144: b'\\x90',\n",
       " 145: b'\\x91',\n",
       " 146: b'\\x92',\n",
       " 147: b'\\x93',\n",
       " 148: b'\\x94',\n",
       " 149: b'\\x95',\n",
       " 150: b'\\x96',\n",
       " 151: b'\\x97',\n",
       " 152: b'\\x98',\n",
       " 153: b'\\x99',\n",
       " 154: b'\\x9a',\n",
       " 155: b'\\x9b',\n",
       " 156: b'\\x9c',\n",
       " 157: b'\\x9d',\n",
       " 158: b'\\x9e',\n",
       " 159: b'\\x9f',\n",
       " 160: b'\\xa0',\n",
       " 161: b'\\xa1',\n",
       " 162: b'\\xa2',\n",
       " 163: b'\\xa3',\n",
       " 164: b'\\xa4',\n",
       " 165: b'\\xa5',\n",
       " 166: b'\\xa6',\n",
       " 167: b'\\xa7',\n",
       " 168: b'\\xa8',\n",
       " 169: b'\\xa9',\n",
       " 170: b'\\xaa',\n",
       " 171: b'\\xab',\n",
       " 172: b'\\xac',\n",
       " 173: b'\\xad',\n",
       " 174: b'\\xae',\n",
       " 175: b'\\xaf',\n",
       " 176: b'\\xb0',\n",
       " 177: b'\\xb1',\n",
       " 178: b'\\xb2',\n",
       " 179: b'\\xb3',\n",
       " 180: b'\\xb4',\n",
       " 181: b'\\xb5',\n",
       " 182: b'\\xb6',\n",
       " 183: b'\\xb7',\n",
       " 184: b'\\xb8',\n",
       " 185: b'\\xb9',\n",
       " 186: b'\\xba',\n",
       " 187: b'\\xbb',\n",
       " 188: b'\\xbc',\n",
       " 189: b'\\xbd',\n",
       " 190: b'\\xbe',\n",
       " 191: b'\\xbf',\n",
       " 192: b'\\xc0',\n",
       " 193: b'\\xc1',\n",
       " 194: b'\\xc2',\n",
       " 195: b'\\xc3',\n",
       " 196: b'\\xc4',\n",
       " 197: b'\\xc5',\n",
       " 198: b'\\xc6',\n",
       " 199: b'\\xc7',\n",
       " 200: b'\\xc8',\n",
       " 201: b'\\xc9',\n",
       " 202: b'\\xca',\n",
       " 203: b'\\xcb',\n",
       " 204: b'\\xcc',\n",
       " 205: b'\\xcd',\n",
       " 206: b'\\xce',\n",
       " 207: b'\\xcf',\n",
       " 208: b'\\xd0',\n",
       " 209: b'\\xd1',\n",
       " 210: b'\\xd2',\n",
       " 211: b'\\xd3',\n",
       " 212: b'\\xd4',\n",
       " 213: b'\\xd5',\n",
       " 214: b'\\xd6',\n",
       " 215: b'\\xd7',\n",
       " 216: b'\\xd8',\n",
       " 217: b'\\xd9',\n",
       " 218: b'\\xda',\n",
       " 219: b'\\xdb',\n",
       " 220: b'\\xdc',\n",
       " 221: b'\\xdd',\n",
       " 222: b'\\xde',\n",
       " 223: b'\\xdf',\n",
       " 224: b'\\xe0',\n",
       " 225: b'\\xe1',\n",
       " 226: b'\\xe2',\n",
       " 227: b'\\xe3',\n",
       " 228: b'\\xe4',\n",
       " 229: b'\\xe5',\n",
       " 230: b'\\xe6',\n",
       " 231: b'\\xe7',\n",
       " 232: b'\\xe8',\n",
       " 233: b'\\xe9',\n",
       " 234: b'\\xea',\n",
       " 235: b'\\xeb',\n",
       " 236: b'\\xec',\n",
       " 237: b'\\xed',\n",
       " 238: b'\\xee',\n",
       " 239: b'\\xef',\n",
       " 240: b'\\xf0',\n",
       " 241: b'\\xf1',\n",
       " 242: b'\\xf2',\n",
       " 243: b'\\xf3',\n",
       " 244: b'\\xf4',\n",
       " 245: b'\\xf5',\n",
       " 246: b'\\xf6',\n",
       " 247: b'\\xf7',\n",
       " 248: b'\\xf8',\n",
       " 249: b'\\xf9',\n",
       " 250: b'\\xfa',\n",
       " 251: b'\\xfb',\n",
       " 252: b'\\xfc',\n",
       " 253: b'\\xfd',\n",
       " 254: b'\\xfe',\n",
       " 255: b'\\xff',\n",
       " 256: b'he',\n",
       " 257: b'hel',\n",
       " 258: b'hell',\n",
       " 259: b'hello'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6dc4973a-e999-4a05-97a2-ea53a0782ca2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'hi'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab[104] + vocab[105]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b82a1e4a-81b8-4366-845e-b07e5c7a851f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{256: (104, 101), 257: (256, 108), 258: (257, 108), 259: (258, 111)}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fb8e2d48-b806-4a01-a32a-c98b1d544b72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(104, 101): 256, (256, 108): 257, (257, 108): 258, (258, 111): 259}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6109eb25-a319-457e-bebe-33784ae29965",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(258, 111)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_freq_pair"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f331be2f-1ac2-40cd-8402-53dfb37bc572",
   "metadata": {},
   "source": [
    "I want achieve:\n",
    "\n",
    "- vocabluary - the map of what pair of bytes were merged: text_corp -> f(x) -> utf8_bytes -> f(x) -> vocab\n",
    "- decode - code, vocab -> f_decode(x, y) -> string\n",
    "- encode - string -> f_encode(x) -> code, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30577ea2-3021-4904-b931-775d751d154f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"“Why?’‘Because it is too small. “Five feet high is the door, and three abreast [first written four abreast] may enter it” say the runes. But Pryftan could not creep in a hole that size, not even when he was a young dragon, certainly not after he had devoured so many maidens of the valley.’‘It seems a pretty big hole,’ piped Bilbo. He loved maps, and in the hall there was a large one of the County Round (where he lived), with all his favourite walks marked on it in red ink. He was so interested he forgot to be shy and keep his mouth shut. ‘How could such an enormous door’ (he was a hobbit, remember) ‘be secret?’‘Lots of ways,’ said Bl[adorthin], ‘but which one of them we don’t know without looking. At the top of the other side of the page there is a list of the dwarves, which includes ‘Gandalf’; and against this my father afterwards wrote in pencil: ‘NB Gandalf was originally chief Dwarf (=Thorin) and Gandalf was called Bladorthin.’ The names of the dwarves in The Hobbit were taken from verses of a very ancient Norse poem called Völuspá, where many dwarf-names are given, and among them Gandalf. The only other difference in this original list is that Oi appears for Ori (in the Völuspá there is the name Ái). – Bladorthin became the name of a long-dead king who is mentioned once in The Hobbit (p. 230) but nowhere else. From what it says on the map I should say that there is a closed door which looks just like the side of the mountain – the ordinary dwarf’s way (I think I am right?)’‘Quite,’ said Gandalf. ‘But this rather alters things. There are fourteen of us – unless you are coming, Bladorthin. I had thought of going up along Running River from the Long Lake – if we can get so far! – and so to the Ruins of Dale Town. But we none of us liked the idea of the Front Gate. The River runs out of that great door, and out of it the Dragon comes too. Far too often.’‘That would have been no good,’ said Bl[adorthin], ‘without a mighty warrior; even a hero. I tried to find one, but I had to fall back (I beg your pardon, but I am sure you will understand – dragon-slaying is not I believe your speciality) – to fall back on little Bilbo [first written Mr Baggins].’‘The burglar,’ said Dwalin. ‘Precisely,’ said Blad[orthin], not allowing Bilbo time to object. ‘I told you last Thursday it would have to be a burglary not a battle, and a burglar I promised to find. I hope no one is going to say I put the sign on the wrong door again.’ He frowned so frightfully at Bilbo that the little man daren’t say anything though he was bursting with questions.‘Warriors are very busy fighting one another in far lands,’ went on Bl[adorthin], ‘and in this neighbour-hood there are none or few [struck out: left, of men, dwarves, elves or hobbits], not to speak of heroes. Swords in the world are mostly blunt, and axes used on trees and shields for dish-covers, and dragons comfortably far-off. But burglary is I think indicated in any case by the presence of the back door.’‘What is your plan?’ they all said. ‘To go to the back door, sit on the step – and think of one – if one does not”\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47671449-8cba-4dc0-81a0-591b3bb5890f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = \"aaaBbaba\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c6e52b3-9bc9-432f-a573-cea0bff9b470",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3212, [226, 128, 156, 87, 104, 121, 63, 226, 128, 153])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = list(text.encode('utf-8'))\n",
    "len(tokens), tokens[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c996e3da-f9d9-4c3c-a650-a91590bdda40",
   "metadata": {},
   "source": [
    "If I want to get vocab, I need to:\n",
    "1. find two most frequent tokens\n",
    "2. add them to map (a, b): new_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35746ce6-c560-4270-8760-80b2ef7f9ccb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((101, 32), 105),\n",
       " ((32, 116), 75),\n",
       " ((116, 104), 73),\n",
       " ((104, 101), 63),\n",
       " ((116, 32), 58)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_freqs(tokens):\n",
    "    freq_stats = {}\n",
    "    for i, j in zip(tokens, tokens[1:]):\n",
    "        freq_stats[(i, j)] = freq_stats.get((i, j), 1) + 1\n",
    "    return freq_stats\n",
    "\n",
    "stats = get_freqs(tokens)\n",
    "sorted(stats.items(), key=lambda x: x[1], reverse=True)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a2d7448-5e89-44c6-802e-8165bad0b374",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(101, 32): 256}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merges = {}\n",
    "max_freq_pair, frq = max(stats.items(), key=lambda x: x[1])\n",
    "merges[max_freq_pair] = 256\n",
    "merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eafce7d6-4567-42b0-8880-0472d2a15d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_tokens_by_parent(tokens, target, parent):\n",
    "    new_tokens = []\n",
    "\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        if i+1 < len(tokens) and (tokens[i], tokens[i+1]) == target:\n",
    "            new_tokens.append(parent)\n",
    "            i += 2\n",
    "        else:\n",
    "            new_tokens.append(tokens[i])\n",
    "            i += 1\n",
    "    \n",
    "    return new_tokens\n",
    "\n",
    "new_tokens = replace_tokens_by_parent(tokens, max_freq_pair, 256)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622f06a2-65eb-472d-bba0-e5d7687400f1",
   "metadata": {},
   "source": [
    "Fit vocab:\n",
    "1. get tokens\n",
    "2. get freqs of tokens\n",
    "3. calculate max frequency pair\n",
    "4. replace\n",
    "5. repeat 2 step K times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b85d7e2-76aa-42cd-bf22-6bba5e542dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_MERGES = 42\n",
    "\n",
    "tokens = list(text.encode('utf-8'))\n",
    "merges = {}\n",
    "\n",
    "for i in range(N_MERGES):\n",
    "    stats = get_freqs(tokens)\n",
    "    max_freq_pair, frq = max(stats.items(), key=lambda x: x[1])\n",
    "    new_token_id = 256 + i\n",
    "    merges[new_token_id] = max_freq_pair\n",
    "    tokens = replace_tokens_by_parent(tokens, max_freq_pair, new_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d7d676c-99bd-4566-a952-55847174474c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5129533678756477"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(text.encode('utf-8'))) / len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b34fc86a-2c10-4fb8-b25b-02513166b493",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'HELLO WORLD!'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = {i:bytes([i]) for i in range(256)}\n",
    "inv_merges = {v: k for k, v in merges.items()}\n",
    "\n",
    "for k, (p0, p1) in merges.items():\n",
    "    vocab[k] = vocab[p0] + vocab[p1]\n",
    "\n",
    "def decode(tokens):\n",
    "    return b\"\".join(vocab.get(t) for t in tokens).decode(\"utf-8\", errors=\"replace\")\n",
    "\n",
    "def encode(string):\n",
    "    tokens = list(string.encode('utf-8'))\n",
    "    while len(tokens) >= 2:\n",
    "        new_tokens = []\n",
    "        i = 0\n",
    "        while i < len(tokens):\n",
    "            if i+1 < len(tokens) and (tokens[i], tokens[i+1]) in inv_merges:\n",
    "                new_tokens.append(inv_merges[tokens[i], tokens[i+1]])\n",
    "                i += 2\n",
    "            else:\n",
    "                new_tokens.append(tokens[i])\n",
    "                i += 1\n",
    "    \n",
    "        is_not_changed = len(tokens) == len(new_tokens)\n",
    "        tokens = new_tokens\n",
    "        if is_not_changed:\n",
    "            break\n",
    "    return tokens\n",
    "\n",
    "decode(encode(\"HELLO WORLD!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4c782e2-3df3-4137-8968-7a95dd65f3fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode(encode(text)) == text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bb687d-723a-4cd4-aaa8-741b6b4159ca",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "\n",
    "https://www.youtube.com/watch?v=zduSFxRajkE&t=3417s&ab_channel=AndrejKarpathy\n",
    "\n",
    "https://github.com/karpathy/minbpe/blob/master/exercise.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e3e3a72-5d94-4971-90b6-e7d2dac87d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(name):\n",
    "    with open(name, \"r\") as file:\n",
    "        return file.read()\n",
    "\n",
    "text = read_file(\"taylorswift.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43b313e7-6de2-4bb4-91e6-d483955d6457",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "class BasicTokenizer:\n",
    "    def __init__(self):\n",
    "        self.vocab, self.merges, self.inv_merges = {}, {}, {}\n",
    "        pass\n",
    "\n",
    "    def get_freqs(self, tokens):\n",
    "        # Using Counter for efficient frequency calculation\n",
    "        token_pairs = zip(tokens, tokens[1:])\n",
    "        return Counter(token_pairs)\n",
    "\n",
    "    def replace_tokens_by_parent(self, tokens, target, parent):\n",
    "        # Improved token replacement for efficiency\n",
    "        new_tokens = []\n",
    "        i = 0\n",
    "        while i < len(tokens):\n",
    "            if i+1 < len(tokens) and (tokens[i], tokens[i+1]) == target:\n",
    "                new_tokens.append(parent)\n",
    "                i += 2\n",
    "            else:\n",
    "                new_tokens.append(tokens[i])\n",
    "                i += 1\n",
    "        return new_tokens\n",
    "    \n",
    "    def train(self, text, vocab_size, verbose=False):\n",
    "        tokens = list(text.encode('utf-8'))\n",
    "        vocab_size -= 256\n",
    "        \n",
    "        merges = {}\n",
    "        \n",
    "        progress_bar = tqdm(range(vocab_size)) if verbose else range(vocab_size)\n",
    "        \n",
    "        for i in progress_bar:\n",
    "            stats = self.get_freqs(tokens)\n",
    "            max_freq_pair, _ = max(stats.items(), key=lambda x: x[1])\n",
    "            new_token_id = 256 + i\n",
    "            merges[new_token_id] = max_freq_pair\n",
    "            tokens = self.replace_tokens_by_parent(tokens, max_freq_pair, new_token_id)\n",
    "        \n",
    "        inv_merges = {v: k for k, v in merges.items()}\n",
    "        vocab = {i: bytes([i]) for i in range(256)}\n",
    "        \n",
    "        for k, (p0, p1) in merges.items():\n",
    "            vocab[k] = vocab[p0] + vocab[p1]\n",
    "        \n",
    "        self.vocab, self.merges, self.inv_merges = vocab, merges, inv_merges\n",
    "        \n",
    "    # def encode(self, text):\n",
    "    #     # given a string text, return the token ids\n",
    "    #     text_bytes = text.encode(\"utf-8\") # raw bytes\n",
    "    #     ids = list(text_bytes) # list of integers in range 0..255\n",
    "    #     while len(ids) >= 2:\n",
    "    #         stats = self.get_freqs(ids)\n",
    "    #         pair = min(stats, key=lambda p: self.inv_merges.get(p, float(\"inf\")))\n",
    "    #         if pair not in self.inv_merges:\n",
    "    #             break\n",
    "    #         idx = self.inv_merges[pair]\n",
    "    #         ids = self.replace_tokens_by_parent(ids, pair, idx)\n",
    "    #     return ids\n",
    "\n",
    "    def encode(self, text):\n",
    "        tokens = list(text.encode('utf-8'))\n",
    "        while len(tokens) >= 2:\n",
    "            new_tokens = []\n",
    "            i = 0\n",
    "            while i < len(tokens):\n",
    "                if i+1 < len(tokens) and (tokens[i], tokens[i+1]) in self.inv_merges:\n",
    "                    new_tokens.append(self.inv_merges[tokens[i], tokens[i+1]])\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_tokens.append(tokens[i])\n",
    "                    i += 1\n",
    "        \n",
    "            is_not_changed = len(tokens) == len(new_tokens)\n",
    "            tokens = new_tokens\n",
    "            if is_not_changed:\n",
    "                break\n",
    "        return tokens\n",
    "        \n",
    "    def decode(self, ids):\n",
    "        return b\"\".join(self.vocab.get(t) for t in ids).decode('utf-8', errors=\"replace\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "efc37401-c4a1-45b6-a9b5-280736fda647",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28b5a481fab44fa99c0ceccbbe3abf88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/244 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_file(name):\n",
    "    with open(name, \"r\", encoding='utf-8') as file:\n",
    "        return file.read()\n",
    "\n",
    "# long_text = read_file('01 - The Fellowship Of The Ring.txt')\n",
    "long_text = read_file('taylorswift.txt')\n",
    "\n",
    "basic_tokenizer = BasicTokenizer()\n",
    "basic_tokenizer.train(long_text, vocab_size=500, verbose=True)\n",
    "\n",
    "basic_tokenizer.decode(basic_tokenizer.encode(long_text)) == long_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f282b13-46bb-474a-b3c1-9b7c3e86ac21",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: b'\\x00',\n",
       " 1: b'\\x01',\n",
       " 2: b'\\x02',\n",
       " 3: b'\\x03',\n",
       " 4: b'\\x04',\n",
       " 5: b'\\x05',\n",
       " 6: b'\\x06',\n",
       " 7: b'\\x07',\n",
       " 8: b'\\x08',\n",
       " 9: b'\\t',\n",
       " 10: b'\\n',\n",
       " 11: b'\\x0b',\n",
       " 12: b'\\x0c',\n",
       " 13: b'\\r',\n",
       " 14: b'\\x0e',\n",
       " 15: b'\\x0f',\n",
       " 16: b'\\x10',\n",
       " 17: b'\\x11',\n",
       " 18: b'\\x12',\n",
       " 19: b'\\x13',\n",
       " 20: b'\\x14',\n",
       " 21: b'\\x15',\n",
       " 22: b'\\x16',\n",
       " 23: b'\\x17',\n",
       " 24: b'\\x18',\n",
       " 25: b'\\x19',\n",
       " 26: b'\\x1a',\n",
       " 27: b'\\x1b',\n",
       " 28: b'\\x1c',\n",
       " 29: b'\\x1d',\n",
       " 30: b'\\x1e',\n",
       " 31: b'\\x1f',\n",
       " 32: b' ',\n",
       " 33: b'!',\n",
       " 34: b'\"',\n",
       " 35: b'#',\n",
       " 36: b'$',\n",
       " 37: b'%',\n",
       " 38: b'&',\n",
       " 39: b\"'\",\n",
       " 40: b'(',\n",
       " 41: b')',\n",
       " 42: b'*',\n",
       " 43: b'+',\n",
       " 44: b',',\n",
       " 45: b'-',\n",
       " 46: b'.',\n",
       " 47: b'/',\n",
       " 48: b'0',\n",
       " 49: b'1',\n",
       " 50: b'2',\n",
       " 51: b'3',\n",
       " 52: b'4',\n",
       " 53: b'5',\n",
       " 54: b'6',\n",
       " 55: b'7',\n",
       " 56: b'8',\n",
       " 57: b'9',\n",
       " 58: b':',\n",
       " 59: b';',\n",
       " 60: b'<',\n",
       " 61: b'=',\n",
       " 62: b'>',\n",
       " 63: b'?',\n",
       " 64: b'@',\n",
       " 65: b'A',\n",
       " 66: b'B',\n",
       " 67: b'C',\n",
       " 68: b'D',\n",
       " 69: b'E',\n",
       " 70: b'F',\n",
       " 71: b'G',\n",
       " 72: b'H',\n",
       " 73: b'I',\n",
       " 74: b'J',\n",
       " 75: b'K',\n",
       " 76: b'L',\n",
       " 77: b'M',\n",
       " 78: b'N',\n",
       " 79: b'O',\n",
       " 80: b'P',\n",
       " 81: b'Q',\n",
       " 82: b'R',\n",
       " 83: b'S',\n",
       " 84: b'T',\n",
       " 85: b'U',\n",
       " 86: b'V',\n",
       " 87: b'W',\n",
       " 88: b'X',\n",
       " 89: b'Y',\n",
       " 90: b'Z',\n",
       " 91: b'[',\n",
       " 92: b'\\\\',\n",
       " 93: b']',\n",
       " 94: b'^',\n",
       " 95: b'_',\n",
       " 96: b'`',\n",
       " 97: b'a',\n",
       " 98: b'b',\n",
       " 99: b'c',\n",
       " 100: b'd',\n",
       " 101: b'e',\n",
       " 102: b'f',\n",
       " 103: b'g',\n",
       " 104: b'h',\n",
       " 105: b'i',\n",
       " 106: b'j',\n",
       " 107: b'k',\n",
       " 108: b'l',\n",
       " 109: b'm',\n",
       " 110: b'n',\n",
       " 111: b'o',\n",
       " 112: b'p',\n",
       " 113: b'q',\n",
       " 114: b'r',\n",
       " 115: b's',\n",
       " 116: b't',\n",
       " 117: b'u',\n",
       " 118: b'v',\n",
       " 119: b'w',\n",
       " 120: b'x',\n",
       " 121: b'y',\n",
       " 122: b'z',\n",
       " 123: b'{',\n",
       " 124: b'|',\n",
       " 125: b'}',\n",
       " 126: b'~',\n",
       " 127: b'\\x7f',\n",
       " 128: b'\\x80',\n",
       " 129: b'\\x81',\n",
       " 130: b'\\x82',\n",
       " 131: b'\\x83',\n",
       " 132: b'\\x84',\n",
       " 133: b'\\x85',\n",
       " 134: b'\\x86',\n",
       " 135: b'\\x87',\n",
       " 136: b'\\x88',\n",
       " 137: b'\\x89',\n",
       " 138: b'\\x8a',\n",
       " 139: b'\\x8b',\n",
       " 140: b'\\x8c',\n",
       " 141: b'\\x8d',\n",
       " 142: b'\\x8e',\n",
       " 143: b'\\x8f',\n",
       " 144: b'\\x90',\n",
       " 145: b'\\x91',\n",
       " 146: b'\\x92',\n",
       " 147: b'\\x93',\n",
       " 148: b'\\x94',\n",
       " 149: b'\\x95',\n",
       " 150: b'\\x96',\n",
       " 151: b'\\x97',\n",
       " 152: b'\\x98',\n",
       " 153: b'\\x99',\n",
       " 154: b'\\x9a',\n",
       " 155: b'\\x9b',\n",
       " 156: b'\\x9c',\n",
       " 157: b'\\x9d',\n",
       " 158: b'\\x9e',\n",
       " 159: b'\\x9f',\n",
       " 160: b'\\xa0',\n",
       " 161: b'\\xa1',\n",
       " 162: b'\\xa2',\n",
       " 163: b'\\xa3',\n",
       " 164: b'\\xa4',\n",
       " 165: b'\\xa5',\n",
       " 166: b'\\xa6',\n",
       " 167: b'\\xa7',\n",
       " 168: b'\\xa8',\n",
       " 169: b'\\xa9',\n",
       " 170: b'\\xaa',\n",
       " 171: b'\\xab',\n",
       " 172: b'\\xac',\n",
       " 173: b'\\xad',\n",
       " 174: b'\\xae',\n",
       " 175: b'\\xaf',\n",
       " 176: b'\\xb0',\n",
       " 177: b'\\xb1',\n",
       " 178: b'\\xb2',\n",
       " 179: b'\\xb3',\n",
       " 180: b'\\xb4',\n",
       " 181: b'\\xb5',\n",
       " 182: b'\\xb6',\n",
       " 183: b'\\xb7',\n",
       " 184: b'\\xb8',\n",
       " 185: b'\\xb9',\n",
       " 186: b'\\xba',\n",
       " 187: b'\\xbb',\n",
       " 188: b'\\xbc',\n",
       " 189: b'\\xbd',\n",
       " 190: b'\\xbe',\n",
       " 191: b'\\xbf',\n",
       " 192: b'\\xc0',\n",
       " 193: b'\\xc1',\n",
       " 194: b'\\xc2',\n",
       " 195: b'\\xc3',\n",
       " 196: b'\\xc4',\n",
       " 197: b'\\xc5',\n",
       " 198: b'\\xc6',\n",
       " 199: b'\\xc7',\n",
       " 200: b'\\xc8',\n",
       " 201: b'\\xc9',\n",
       " 202: b'\\xca',\n",
       " 203: b'\\xcb',\n",
       " 204: b'\\xcc',\n",
       " 205: b'\\xcd',\n",
       " 206: b'\\xce',\n",
       " 207: b'\\xcf',\n",
       " 208: b'\\xd0',\n",
       " 209: b'\\xd1',\n",
       " 210: b'\\xd2',\n",
       " 211: b'\\xd3',\n",
       " 212: b'\\xd4',\n",
       " 213: b'\\xd5',\n",
       " 214: b'\\xd6',\n",
       " 215: b'\\xd7',\n",
       " 216: b'\\xd8',\n",
       " 217: b'\\xd9',\n",
       " 218: b'\\xda',\n",
       " 219: b'\\xdb',\n",
       " 220: b'\\xdc',\n",
       " 221: b'\\xdd',\n",
       " 222: b'\\xde',\n",
       " 223: b'\\xdf',\n",
       " 224: b'\\xe0',\n",
       " 225: b'\\xe1',\n",
       " 226: b'\\xe2',\n",
       " 227: b'\\xe3',\n",
       " 228: b'\\xe4',\n",
       " 229: b'\\xe5',\n",
       " 230: b'\\xe6',\n",
       " 231: b'\\xe7',\n",
       " 232: b'\\xe8',\n",
       " 233: b'\\xe9',\n",
       " 234: b'\\xea',\n",
       " 235: b'\\xeb',\n",
       " 236: b'\\xec',\n",
       " 237: b'\\xed',\n",
       " 238: b'\\xee',\n",
       " 239: b'\\xef',\n",
       " 240: b'\\xf0',\n",
       " 241: b'\\xf1',\n",
       " 242: b'\\xf2',\n",
       " 243: b'\\xf3',\n",
       " 244: b'\\xf4',\n",
       " 245: b'\\xf5',\n",
       " 246: b'\\xf6',\n",
       " 247: b'\\xf7',\n",
       " 248: b'\\xf8',\n",
       " 249: b'\\xf9',\n",
       " 250: b'\\xfa',\n",
       " 251: b'\\xfb',\n",
       " 252: b'\\xfc',\n",
       " 253: b'\\xfd',\n",
       " 254: b'\\xfe',\n",
       " 255: b'\\xff',\n",
       " 256: b'e ',\n",
       " 257: b', ',\n",
       " 258: b'd ',\n",
       " 259: b'. ',\n",
       " 260: b'r ',\n",
       " 261: b'20',\n",
       " 262: b's ',\n",
       " 263: b'in',\n",
       " 264: b'on',\n",
       " 265: b'ri',\n",
       " 266: b't ',\n",
       " 267: b'th',\n",
       " 268: b'ed ',\n",
       " 269: b', 20',\n",
       " 270: b'an',\n",
       " 271: b'ar',\n",
       " 272: b'er ',\n",
       " 273: b'y ',\n",
       " 274: b'al',\n",
       " 275: b'the ',\n",
       " 276: b'ved ',\n",
       " 277: b'wi',\n",
       " 278: b'er',\n",
       " 279: b'on ',\n",
       " 280: b'wif',\n",
       " 281: b'Re',\n",
       " 282: b'Swif',\n",
       " 283: b'or ',\n",
       " 284: b'ch',\n",
       " 285: b', 201',\n",
       " 286: b'om',\n",
       " 287: b'ber ',\n",
       " 288: b' the ',\n",
       " 289: b'ay',\n",
       " 290: b'en',\n",
       " 291: b'or',\n",
       " 292: b'al ',\n",
       " 293: b'em',\n",
       " 294: b'.\\n',\n",
       " 295: b'rie',\n",
       " 296: b'ing',\n",
       " 297: b', 202',\n",
       " 298: b'ti',\n",
       " 299: b'ayl',\n",
       " 300: b'\". ',\n",
       " 301: b'll',\n",
       " 302: b'Tayl',\n",
       " 303: b'trie',\n",
       " 304: b'.\\n ',\n",
       " 305: b'to',\n",
       " 306: b'. Re',\n",
       " 307: b'. Retrie',\n",
       " 308: b'. Retrieved ',\n",
       " 309: b'Taylor ',\n",
       " 310: b'es',\n",
       " 311: b'Taylor Swif',\n",
       " 312: b'us',\n",
       " 313: b'rom',\n",
       " 314: b'ember ',\n",
       " 315: b'). ',\n",
       " 316: b'Ar',\n",
       " 317: b'from',\n",
       " 318: b'). \"',\n",
       " 319: b'and ',\n",
       " 320: b're',\n",
       " 321: b'ou',\n",
       " 322: b'ori',\n",
       " 323: b'of',\n",
       " 324: b'gin',\n",
       " 325: b'ing ',\n",
       " 326: b'chi',\n",
       " 327: b'] ',\n",
       " 328: b'ginal ',\n",
       " 329: b'from the ',\n",
       " 330: b'original ',\n",
       " 331: b'he ',\n",
       " 332: b'Archi',\n",
       " 333: b'Archived ',\n",
       " 334: b'from the original ',\n",
       " 335: b'Archived from the original ',\n",
       " 336: b'Archived from the original on ',\n",
       " 337: b'. Archived from the original on ',\n",
       " 338: b'a ',\n",
       " 339: b'st',\n",
       " 340: b'ic',\n",
       " 341: b'.[',\n",
       " 342: b'ec',\n",
       " 343: b'ill',\n",
       " 344: b\"'s \",\n",
       " 345: b'Taylor Swift ',\n",
       " 346: b'ov',\n",
       " 347: b'at',\n",
       " 348: b'as ',\n",
       " 349: b'es ',\n",
       " 350: b'Ju',\n",
       " 351: b'of ',\n",
       " 352: b'to ',\n",
       " 353: b'um',\n",
       " 354: b'The ',\n",
       " 355: b'ard',\n",
       " 356: b'in ',\n",
       " 357: b'an ',\n",
       " 358: b'el',\n",
       " 359: b', 2023',\n",
       " 360: b'ary ',\n",
       " 361: b'th ',\n",
       " 362: b'am',\n",
       " 363: b'ly ',\n",
       " 364: b'op',\n",
       " 365: b'Taylor Swift',\n",
       " 366: b'tr',\n",
       " 367: b'is',\n",
       " 368: b'her ',\n",
       " 369: b'o ',\n",
       " 370: b'uary ',\n",
       " 371: b'Nov',\n",
       " 372: b'usic',\n",
       " 373: b'November ',\n",
       " 374: b'ew',\n",
       " 375: b'at ',\n",
       " 376: b'l ',\n",
       " 377: b': ',\n",
       " 378: b'bo',\n",
       " 379: b'Swift ',\n",
       " 380: b'Dec',\n",
       " 381: b'it',\n",
       " 382: b'ig',\n",
       " 383: b'Bill',\n",
       " 384: b'10',\n",
       " 385: b'as',\n",
       " 386: b'ong',\n",
       " 387: b'Oc',\n",
       " 388: b'ati',\n",
       " 389: b'St',\n",
       " 390: b'Octo',\n",
       " 391: b'October ',\n",
       " 392: b'ac',\n",
       " 393: b'ow',\n",
       " 394: b'December ',\n",
       " 395: b'Billbo',\n",
       " 396: b'ad',\n",
       " 397: b'le',\n",
       " 398: b'ur',\n",
       " 399: b'for ',\n",
       " 400: b' (',\n",
       " 401: b', 2022',\n",
       " 402: b'ug',\n",
       " 403: b'ch ',\n",
       " 404: b'st ',\n",
       " 405: b'oun',\n",
       " 406: b'bum',\n",
       " 407: b'ol',\n",
       " 408: b'ust ',\n",
       " 409: b'eb',\n",
       " 410: b'Ma',\n",
       " 411: b'July ',\n",
       " 412: b'). \"Taylor Swift ',\n",
       " 413: b'k ',\n",
       " 414: b'ers',\n",
       " 415: b'][',\n",
       " 416: b'Aug',\n",
       " 417: b'August ',\n",
       " 418: b'id',\n",
       " 419: b', 2021',\n",
       " 420: b'me',\n",
       " 421: b'ep',\n",
       " 422: b'201',\n",
       " 423: b'23',\n",
       " 424: b', 2012',\n",
       " 425: b'ear',\n",
       " 426: b', 2020',\n",
       " 427: b'In',\n",
       " 428: b'fi',\n",
       " 429: b'ne ',\n",
       " 430: b'Billboard',\n",
       " 431: b'rit',\n",
       " 432: b'hi',\n",
       " 433: b'usic ',\n",
       " 434: b'.\\n \"',\n",
       " 435: b'New',\n",
       " 436: b'di',\n",
       " 437: b'Ap',\n",
       " 438: b', 2019',\n",
       " 439: b'ro',\n",
       " 440: b\"' \",\n",
       " 441: b's, ',\n",
       " 442: b'June ',\n",
       " 443: b'of the ',\n",
       " 444: b'cor',\n",
       " 445: b'21',\n",
       " 446: b'19',\n",
       " 447: b'im',\n",
       " 448: b'en ',\n",
       " 449: b'ebr',\n",
       " 450: b'ent',\n",
       " 451: b'oll',\n",
       " 452: b'Mar',\n",
       " 453: b'ric',\n",
       " 454: b'with ',\n",
       " 455: b',[',\n",
       " 456: b'Febr',\n",
       " 457: b'February ',\n",
       " 458: b\"Taylor Swift's \",\n",
       " 459: b'\". Billboard',\n",
       " 460: b'ea',\n",
       " 461: b', 2016',\n",
       " 462: b'ept',\n",
       " 463: b'May ',\n",
       " 464: b', 2015',\n",
       " 465: b'Apri',\n",
       " 466: b'April ',\n",
       " 467: b'le ',\n",
       " 468: b'Aw',\n",
       " 469: b'ation',\n",
       " 470: b'Sept',\n",
       " 471: b'September ',\n",
       " 472: b'ra',\n",
       " 473: b'album',\n",
       " 474: b'Ch',\n",
       " 475: b've ',\n",
       " 476: b'est ',\n",
       " 477: b'Jan',\n",
       " 478: b'22',\n",
       " 479: b'January ',\n",
       " 480: b'ountr',\n",
       " 481: b'igh',\n",
       " 482: b'\". The ',\n",
       " 483: b', 2023.\\n ',\n",
       " 484: b'13',\n",
       " 485: b'Al',\n",
       " 486: b'et',\n",
       " 487: b'ess',\n",
       " 488: b'March ',\n",
       " 489: b'ut',\n",
       " 490: b'writ',\n",
       " 491: b'lo',\n",
       " 492: b'song',\n",
       " 493: b'\\xe2\\x80',\n",
       " 494: b'ard ',\n",
       " 495: b'0 ',\n",
       " 496: b'ul',\n",
       " 497: b'24',\n",
       " 498: b'is ',\n",
       " 499: b'tic'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basic_tokenizer.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d87dffa2-cda3-4184-8b23-f7e949141544",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span style=\"background-color: #cdbaf7;\">H</span><span style=\"background-color: #bad9f7;\">el</span><span style=\"background-color: #f7badb;\">lo</span><span style=\"background-color: #bbf7ba;\">!</span><span style=\"background-color: #bff7ba;\"> </span><span style=\"background-color: #cdbaf7;\">H</span><span style=\"background-color: #f7bad3;\">ow</span><span style=\"background-color: #bff7ba;\"> </span><span style=\"background-color: #cabaf7;\">ar</span><span style=\"background-color: #bae1f7;\">e </span><span style=\"background-color: #e7f7ba;\">y</span><span style=\"background-color: #e7f7ba;\">ou</span><span style=\"background-color: #bac7f7;\">?</span><span style=\"background-color: #bff7ba;\"> </span><span style=\"background-color: #baddf7;\">February </span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# !pip install seaborn\n",
    "from IPython.display import display, HTML\n",
    "import colorsys\n",
    "\n",
    "def soft_pastel_colors(num_colors):\n",
    "    # Generate colors in HSL space and convert to RGB, then to HEX for softer, muted tones\n",
    "    colors = []\n",
    "    for i in range(num_colors):\n",
    "        # HSL: Hue, Saturation (lower for softness), Lightness (higher for pastel)\n",
    "        hue = i / num_colors\n",
    "        # saturation = 0.35  # Lower saturation for softness\n",
    "        saturation = 0.80  # Lower saturation for softness\n",
    "        lightness = 0.85  # Higher lightness for pastel\n",
    "        rgb = colorsys.hls_to_rgb(hue, lightness, saturation)\n",
    "        hex_color = \"#{:02x}{:02x}{:02x}\".format(int(rgb[0]*255), int(rgb[1]*255), int(rgb[2]*255))\n",
    "        colors.append(hex_color)\n",
    "    return colors\n",
    "\n",
    "def get_color(token, color_list):\n",
    "    return color_list[token % len(color_list)]\n",
    "\n",
    "def print_colored_text(text, color_list):\n",
    "    encoded_text = basic_tokenizer.encode(text)\n",
    "    html_string = ''\n",
    "    \n",
    "    for token in encoded_text:\n",
    "        color = get_color(token, color_list)\n",
    "        decoded_char = basic_tokenizer.decode([token])  # Assuming this returns a string\n",
    "        # print(f\"{token} : {decoded_char}\")\n",
    "        html_string += f'<span style=\"background-color: {color};\">{decoded_char}</span>'\n",
    "    \n",
    "    display(HTML(html_string))\n",
    "\n",
    "# Example usage\n",
    "# color_list = ['#FFDDDD', '#DDFFDD', '#DDDDFF', '#FFFFDD', '#DDFFFF', '#FFDDFF']\n",
    "color_list =  soft_pastel_colors(100)\n",
    "\n",
    "# This will display the colored text in a Jupyter Notebook cell\n",
    "print_colored_text(\"Hello! How are you? February \", color_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "281479f6-459c-492b-b6cd-e42b565cd9e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[72, 358, 491, 33, 32, 72, 393, 32, 271, 256, 121, 321, 63, 32, 457]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basic_tokenizer.encode(\"Hello! How are you? February \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab312d47-4b63-44fd-8bba-1482036094db",
   "metadata": {},
   "source": [
    "# Make regex tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "1bf25846-cdbb-4ab5-bcfe-bd4c8e4be161",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9128081bc01742508cb2bd2e70ae0afd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/44 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import regex as re\n",
    "\n",
    "# GPT4_SPLIT_PATTERN = r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\"\n",
    "\n",
    "# some_string = \"Hello! How are you? February \"\n",
    "# splitted_text = re.findall(GPT4_SPLIT_PATTERN, some_string)\n",
    "\n",
    "# tokens = [list(i.encode('utf-8')) for i in splitted_text]\n",
    "\n",
    "# vocab_size = 300\n",
    "# vocab_size -= 256\n",
    "\n",
    "# merges = {}\n",
    "# progress_bar = tqdm(range(vocab_size)) if True else range(vocab_size)\n",
    "\n",
    "# for i in progress_bar:\n",
    "#     stats = Counter()\n",
    "    \n",
    "#     for piece in tokens:\n",
    "#         for pair in zip(piece[:-1], piece[1:]):\n",
    "#             stats[pair] += 1\n",
    "    \n",
    "#     if not stats: break\n",
    "    \n",
    "#     max_pair = max(stats, key=lambda x: stats[x])\n",
    "#     new_token_id = 256 + i\n",
    "#     # new_token_id = len(stats)\n",
    "#     merges[new_token_id] = max_pair\n",
    "\n",
    "#     for j, piece in enumerate(tokens):\n",
    "#         tokens[j] = basic_tokenizer.replace_tokens_by_parent(piece, max_pair, new_token_id)\n",
    "\n",
    "# inv_merges = {v: k for k, v in merges.items()}\n",
    "# vocab = {i: bytes([i]) for i in range(256)}\n",
    "\n",
    "# for k, (p0, p1) in merges.items():\n",
    "#     vocab[k] = vocab[p0] + vocab[p1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2948616-f3f8-4152-a198-b44ac531edcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "import colorsys\n",
    "\n",
    "def soft_pastel_colors(num_colors):\n",
    "    colors = []\n",
    "    for i in range(num_colors):\n",
    "        # HSL: Hue, Saturation (lower for softness), Lightness (higher for pastel)\n",
    "        hue = i / num_colors\n",
    "        # saturation = 0.35  # Lower saturation for softness\n",
    "        saturation = 0.80  # Lower saturation for softness\n",
    "        lightness = 0.85  # Higher lightness for pastel\n",
    "        rgb = colorsys.hls_to_rgb(hue, lightness, saturation)\n",
    "        hex_color = \"#{:02x}{:02x}{:02x}\".format(int(rgb[0]*255), int(rgb[1]*255), int(rgb[2]*255))\n",
    "        colors.append(hex_color)\n",
    "    return colors\n",
    "\n",
    "def get_color(token, color_list):\n",
    "    return color_list[token % len(color_list)]\n",
    "\n",
    "def print_colored_text(text, color_list, tokenizer):\n",
    "    encoded_text = tokenizer.encode(text)\n",
    "    html_string = ''\n",
    "    \n",
    "    for token in encoded_text:\n",
    "        color = get_color(token, color_list)\n",
    "        decoded_char = tokenizer.decode([token])  # Assuming this returns a string\n",
    "        # print(f\"{token} : {decoded_char}\")\n",
    "        html_string += f'<span style=\"background-color: {color};\">{decoded_char}</span>'\n",
    "    \n",
    "    display(HTML(html_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "390c2b1d-8636-4cae-89a2-a93e38bf31f2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BasicTokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Counter\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mregex\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mRegenxTokenizer\u001b[39;00m(\u001b[43mBasicTokenizer\u001b[49m):\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m      8\u001b[0m         \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'BasicTokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "import regex as re\n",
    "\n",
    "class RegenxTokenizer(BasicTokenizer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.GPT4_SPLIT_PATTERN = r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\"\n",
    "        self.vocab, self.merges, self.inv_merges = {}, {}, {}\n",
    "\n",
    "    def get_freqs(self, tokens):\n",
    "        # Using Counter for efficient frequency calculation\n",
    "        token_pairs = zip(tokens, tokens[1:])\n",
    "        return Counter(token_pairs)\n",
    "\n",
    "    def replace_tokens_by_parent(self, tokens, target, parent):\n",
    "        # Improved token replacement for efficiency\n",
    "        new_tokens = []\n",
    "        i = 0\n",
    "        while i < len(tokens):\n",
    "            if i+1 < len(tokens) and (tokens[i], tokens[i+1]) == target:\n",
    "                new_tokens.append(parent)\n",
    "                i += 2\n",
    "            else:\n",
    "                new_tokens.append(tokens[i])\n",
    "                i += 1\n",
    "        return new_tokens\n",
    "    \n",
    "    def train(self, text, vocab_size, verbose=False):\n",
    "        splitted_text = re.findall(self.GPT4_SPLIT_PATTERN, text)\n",
    "        \n",
    "        tokens = [list(i.encode('utf-8')) for i in splitted_text]\n",
    "        \n",
    "        vocab_size -= 256\n",
    "        \n",
    "        merges = {}\n",
    "        progress_bar = tqdm(range(vocab_size)) if True else range(vocab_size)\n",
    "        \n",
    "        for i in progress_bar:\n",
    "            stats = Counter()\n",
    "            \n",
    "            for piece in tokens:\n",
    "                for pair in zip(piece[:-1], piece[1:]):\n",
    "                    stats[pair] += 1\n",
    "            \n",
    "            if not stats: break\n",
    "            \n",
    "            max_pair = max(stats, key=lambda x: stats[x])\n",
    "            new_token_id = 256 + i\n",
    "            merges[new_token_id] = max_pair\n",
    "        \n",
    "            for j, piece in enumerate(tokens):\n",
    "                tokens[j] = self.replace_tokens_by_parent(piece, max_pair, new_token_id)\n",
    "        \n",
    "        inv_merges = {v: k for k, v in merges.items()}\n",
    "        vocab = {i: bytes([i]) for i in range(256)}\n",
    "        \n",
    "        for k, (p0, p1) in merges.items():\n",
    "            vocab[k] = vocab[p0] + vocab[p1]\n",
    "        \n",
    "        self.vocab, self.merges, self.inv_merges = vocab, merges, inv_merges\n",
    "        \n",
    "    def encode_piece(self, text):\n",
    "        tokens = list(text.encode('utf-8'))\n",
    "        while len(tokens) >= 2:\n",
    "            new_tokens = []\n",
    "            i = 0\n",
    "            while i < len(tokens):\n",
    "                if i+1 < len(tokens) and (tokens[i], tokens[i+1]) in self.inv_merges:\n",
    "                    new_tokens.append(self.inv_merges[tokens[i], tokens[i+1]])\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_tokens.append(tokens[i])\n",
    "                    i += 1\n",
    "        \n",
    "            is_not_changed = len(tokens) == len(new_tokens)\n",
    "            tokens = new_tokens\n",
    "            if is_not_changed:\n",
    "                break\n",
    "        return tokens\n",
    "\n",
    "    def encode_piece2(self, text):\n",
    "        # given a string text, return the token ids\n",
    "        text_bytes = text.encode(\"utf-8\") # raw bytes\n",
    "        ids = list(text_bytes) # list of integers in range 0..255\n",
    "        while len(ids) >= 2:\n",
    "            stats = self.get_freqs(ids)\n",
    "            pair = min(stats, key=lambda p: self.inv_merges.get(p, float(\"inf\")))\n",
    "            if pair not in self.inv_merges:\n",
    "                break\n",
    "            idx = self.inv_merges[pair]\n",
    "            ids = self.replace_tokens_by_parent(ids, pair, idx)\n",
    "        return ids\n",
    "\n",
    "    def encode(self, text):\n",
    "        splitted_text = re.findall(self.GPT4_SPLIT_PATTERN, text)\n",
    "        return [item for i in splitted_text for item in self.encode_piece2(i)]\n",
    "        \n",
    "    def decode(self, ids):\n",
    "        return b\"\".join(self.vocab.get(t) for t in ids).decode('utf-8', errors=\"replace\")\n",
    "\n",
    "def read_file(name):\n",
    "    with open(name, \"r\", encoding='utf-8') as file:\n",
    "        return file.read()\n",
    "\n",
    "# long_text = read_file('taylorswift.txt')\n",
    "\n",
    "long_text = \"\"\"\n",
    "Three Rings for the Elven-kings under the sky,\n",
    "Seven for the Dwarf-lords in their halls of stone,\n",
    "Nine for Mortal Men doomed to die,\n",
    "One for the Dark Lord on his dark throne\n",
    "In the Land of Mordor where the Shadows lie.\n",
    "      One Ring to rule them all, One Ring to find them,\n",
    "      One Ring to bring them all, and in the darkness bind them\n",
    "In the Land of Mordor where the Shadows lie.\n",
    "\n",
    "hello world!!!? (안녕하세요!) lol123 😉\n",
    "\"\"\"\n",
    "\n",
    "tokenizer = RegenxTokenizer()\n",
    "tokenizer.train(long_text, vocab_size=1000, verbose=True)\n",
    "\n",
    "tokenizer.decode(tokenizer.encode(long_text)) == long_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f69338d-9f60-4ddb-8051-3ea57d8d7917",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "0503ebeb-9217-4856-a4bb-b947c96e5c70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"white-space: pre-wrap;\"><span style=\"background-color: #f7deba;\">\n",
       "</span><span style=\"background-color: #ebf7ba;\">Three</span><span style=\"background-color: #e7f7ba;\"> Rings</span><span style=\"background-color: #bbbaf7;\"> for</span><span style=\"background-color: #bad9f7;\"> the</span><span style=\"background-color: #dcf7ba;\"> Elven</span><span style=\"background-color: #d1f7ba;\">-kings</span><span style=\"background-color: #c2f7ba;\"> under</span><span style=\"background-color: #bad9f7;\"> the</span><span style=\"background-color: #bbf7ba;\"> sky</span><span style=\"background-color: #c2baf7;\">,\n",
       "</span><span style=\"background-color: #baf7c0;\">Seven</span><span style=\"background-color: #bbbaf7;\"> for</span><span style=\"background-color: #bad9f7;\"> the</span><span style=\"background-color: #baf7cb;\"> Dwarf</span><span style=\"background-color: #baf7d6;\">-lords</span><span style=\"background-color: #f7bad3;\"> in</span><span style=\"background-color: #baf7dd;\"> their</span><span style=\"background-color: #baf7e4;\"> halls</span><span style=\"background-color: #f2baf7;\"> of</span><span style=\"background-color: #baf7ec;\"> stone</span><span style=\"background-color: #c2baf7;\">,\n",
       "</span><span style=\"background-color: #baf7f3;\">Nine</span><span style=\"background-color: #bbbaf7;\"> for</span><span style=\"background-color: #baf0f7;\"> Mortal</span><span style=\"background-color: #baecf7;\"> Men</span><span style=\"background-color: #bad9f7;\"> doomed</span><span style=\"background-color: #dcbaf7;\"> to</span><span style=\"background-color: #bad6f7;\"> die</span><span style=\"background-color: #c2baf7;\">,\n",
       "</span><span style=\"background-color: #e0baf7;\">One</span><span style=\"background-color: #bbbaf7;\"> for</span><span style=\"background-color: #bad9f7;\"> the</span><span style=\"background-color: #bad2f7;\"> Dark</span><span style=\"background-color: #bacef7;\"> Lord</span><span style=\"background-color: #bacbf7;\"> on</span><span style=\"background-color: #bac3f7;\"> his</span><span style=\"background-color: #f7bac8;\"> dark</span><span style=\"background-color: #bbbaf7;\"> throne</span><span style=\"background-color: #f7deba;\">\n",
       "</span><span style=\"background-color: #f7bac5;\">In</span><span style=\"background-color: #bad9f7;\"> the</span><span style=\"background-color: #f7bac1;\"> Land</span><span style=\"background-color: #f2baf7;\"> of</span><span style=\"background-color: #f7baba;\"> Mordor</span><span style=\"background-color: #f7c1ba;\"> where</span><span style=\"background-color: #bad9f7;\"> the</span><span style=\"background-color: #f7dbba;\"> Shadows</span><span style=\"background-color: #f7deba;\"> lie</span><span style=\"background-color: #f7e2ba;\">.\n",
       "</span><span style=\"background-color: #f7e9ba;\">     </span><span style=\"background-color: #f7bae2;\"> One</span><span style=\"background-color: #babcf7;\"> Ring</span><span style=\"background-color: #dcbaf7;\"> to</span><span style=\"background-color: #cabaf7;\"> rule</span><span style=\"background-color: #e3baf7;\"> them</span><span style=\"background-color: #f7edba;\"> all</span><span style=\"background-color: #baf7e1;\">,</span><span style=\"background-color: #f7bae2;\"> One</span><span style=\"background-color: #babcf7;\"> Ring</span><span style=\"background-color: #dcbaf7;\"> to</span><span style=\"background-color: #cdbaf7;\"> find</span><span style=\"background-color: #e3baf7;\"> them</span><span style=\"background-color: #c2baf7;\">,\n",
       "</span><span style=\"background-color: #f7e9ba;\">     </span><span style=\"background-color: #f7bae2;\"> One</span><span style=\"background-color: #babcf7;\"> Ring</span><span style=\"background-color: #dcbaf7;\"> to</span><span style=\"background-color: #d5baf7;\"> bring</span><span style=\"background-color: #e3baf7;\"> them</span><span style=\"background-color: #f7edba;\"> all</span><span style=\"background-color: #baf7e1;\">,</span><span style=\"background-color: #d8baf7;\"> and</span><span style=\"background-color: #f7bad3;\"> in</span><span style=\"background-color: #bad9f7;\"> the</span><span style=\"background-color: #e3baf7;\"> darkness</span><span style=\"background-color: #e7baf7;\"> bind</span><span style=\"background-color: #e3baf7;\"> them</span><span style=\"background-color: #f7deba;\">\n",
       "</span><span style=\"background-color: #f7bac5;\">In</span><span style=\"background-color: #bad9f7;\"> the</span><span style=\"background-color: #f7bac1;\"> Land</span><span style=\"background-color: #f2baf7;\"> of</span><span style=\"background-color: #f7baba;\"> Mordor</span><span style=\"background-color: #f7c1ba;\"> where</span><span style=\"background-color: #bad9f7;\"> the</span><span style=\"background-color: #f7dbba;\"> Shadows</span><span style=\"background-color: #f7deba;\"> lie</span><span style=\"background-color: #ebbaf7;\">.\n",
       "\n",
       "</span><span style=\"background-color: #f6baf7;\">hello</span><span style=\"background-color: #f7baed;\"> world</span><span style=\"background-color: #f7bae6;\">!!!?</span><span style=\"background-color: #f7bae2;\"> (</span><span style=\"background-color: #f7c5ba;\">안녕하세요</span><span style=\"background-color: #f7c8ba;\">!)</span><span style=\"background-color: #f7d0ba;\"> lol</span><span style=\"background-color: #f7d7ba;\">123</span><span style=\"background-color: #f7e9ba;\"> 😉\n",
       "</span></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of tokens: 98, string length: 420, compression rate: 4.285714285714286\n"
     ]
    }
   ],
   "source": [
    "color_list =  soft_pastel_colors(100)\n",
    "\n",
    "test_text = \"\"\"\n",
    "Three Rings for the Elven-kings under the sky,\n",
    "Seven for the Dwarf-lords in their halls of stone,\n",
    "Nine for Mortal Men doomed to die,\n",
    "One for the Dark Lord on his dark throne\n",
    "In the Land of Mordor where the Shadows lie.\n",
    "      One Ring to rule them all, One Ring to find them,\n",
    "      One Ring to bring them all, and in the darkness bind them\n",
    "In the Land of Mordor where the Shadows lie.\n",
    "\n",
    "hello world!!!? (안녕하세요!) lol123 😉\n",
    "\"\"\"\n",
    "\n",
    "def print_colored_text(text, color_list, tokenizer):\n",
    "    encoded_text = tokenizer.encode(text)\n",
    "    html_string = '<div style=\"white-space: pre-wrap;\">'\n",
    "    \n",
    "    for token in encoded_text:\n",
    "        color = get_color(token, color_list)\n",
    "        decoded_char = tokenizer.decode([token])  # Assuming this returns a string including spaces\n",
    "        # Escaping HTML if necessary\n",
    "        decoded_char = decoded_char.replace('&', '&amp;').replace('<', '&lt;').replace('>', '&gt;')\n",
    "        html_string += f'<span style=\"background-color: {color};\">{decoded_char}</span>'\n",
    "    \n",
    "    html_string += '</div>'\n",
    "    display(HTML(html_string))\n",
    "\n",
    "    n_tokens = len(encoded_text)\n",
    "    n_chars = len(text)\n",
    "\n",
    "    print(f\"Length of tokens: {n_tokens}, string length: {n_chars}, compression rate: {n_chars / n_tokens}\")\n",
    "\n",
    "print_colored_text(test_text, color_list, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c3a54d-7994-4e62-9a39-6a2f54daad07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-13.m103",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-13:m103"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
